---
title: "Selected Publications"
permalink: /publications/
author_profile: true
redirect_from:
  - /md/
  - /markdown.html
---


<!DOCTYPE html>

<html lang="en" class="SYSU"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

 

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    
    <title>Publications</title>
    
    <!-- Bootstrap core CSS -->
    <link href="./Basic/bootstrap.min.css" rel="stylesheet">
    
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./Basic/ie10-viewport-bug-workaround.css" rel="stylesheet">
    <!-- Fontawesome -->
    <link rel="stylesheet" href="./Basic/font-awesome.min.css">
    
    <!-- Custom styles for this template -->
    <link href="./Basic/starter-template.css" rel="stylesheet">
    <!-- My own style -->
    <link href="./Basic/project_style.css" rel="stylesheet">
    
    <!-- Fonts -->
    <link href="./Basic/css" rel="stylesheet" type="text/css">
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script async="" src="./Basic/analytics.js"></script><script src="./Basic/ie-emulation-modes-warning.js"></script>
    <!-- Google analytics snippet -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-96369953-1', 'auto');
      ga('send', 'pageview');
    </script>

<!--     <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-96369953-3', 'auto');
      ga('send', 'pageview');
    </script> -->

  </head>
  <body data-gr-c-s-loaded="true">

    <div class="container">
      <div class="row">
        <!-- <div style="text-align:left"><B><B><h1>Publication List</B></B></strong></div> -->
        &nbsp;
        &nbsp;
        &nbsp;      
        &nbsp; 
        <div>
        <h1 style="font-size:1em; "> Representive Papers </h1>

        <p class="pointers_sub">
        <small><B>3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding</B></small>.<br>
        <small>Xiaoye Wang (intern from University of Cambridge), Chen Tang, Xiangyu Yue, <B>Wei-Hong Li</B></small>.<br>
        <small style="color:DarkRed">Preprint</small> <small>/ <a style="color:DarkRed" href="https://arxiv.org/abs/2511.20646">arXiv</a> / <a style="color:DarkRed" href="https://github.com/WeiHongLee/CrossView3DMTL">Code</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via Self-Discovering Latent Directions</B></small>.<br>
        <small>Yilei Jiang, <B>Wei-Hong Li</B>, Yiyuan Zhang, Minghong Cai, Xiangyu Yue</small>.<br>
        <small style="color:DarkRed">ICCV 2025</small> <small>/ <a style="color:DarkRed" href="https://www.arxiv.org/abs/2412.18810">arXiv</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>UniSTD: Towards Unified Spatio-Temporal Prediction across Diverse Disciplines</B></small>.<br>
        <small>Chen Tang, Xinzhu Ma, Encheng Su, Xiufeng Song, Xiaohong Liu, <B><I>Wei-Hong Li</I></B>, Lei Bai, Wanli Ouyang, Xiangyu Yue</small>.<br>
        <small style="color:DarkRed">CVPR 2025</small> <small>/ <a style="color:DarkRed" href="https://arxiv.org/abs/2503.20748">arXiv</a> / <a style="color:DarkRed" href="https://github.com/1hunters/UniSTD">Code</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Bifröst: 3D-Aware Image compositing with Language Instructions</B></small>.<br>
        <small>Lingxiao Li, Kaixiong Gong, <B><I>Wei-Hong Li</I></B>(Corresponding author), Xili Dai, Tao Chen, Xiaojun Yuan, Xiangyu Yue(Corresponding author)</small>.<br>
        <small style="color:DarkRed">Neurips 2024</small> <small>/ <a style="color:DarkRed" href="https://arxiv.org/pdf/2410.19079">arXiv</a> / <a style="color:DarkRed" href="https://github.com/lingxiao-li/Bifrost">Code</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Multi-task Learning with 3D-Aware Regularization</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Steven McDonagh, Ales Leonardis, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">ICLR 2024</small> <small>/ <a style="color:DarkRed" href="https://openreview.net/pdf?id=TwBY17Hgiy">OpenReview</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2310.00986.pdf">arXiv</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/MTPSL">Code</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Universal Representations: A Unified Look at Multiple Task and Domain Learning</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Xialei Liu, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">IJCV 2023</small> <small>/ <a style="color:DarkRed" href="https://arxiv.org/pdf/2204.02744.pdf">arXiv</a> / <a style="color:DarkRed" href="https://link.springer.com/article/10.1007/s11263-023-01931-6">Springer</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/UniversalRepresentations">Code</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Learning universal representations across tasks and domains</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B></small><br>
        <small style="color:DarkRed">University of Edinburgh 2022</small> <small>/ <a style="color:DarkRed" href="https://era.ed.ac.uk/handle/1842/39625">PhD Thesis</a> </small><br>
        <small><b><a style="color:DarkRed" href="https://www.bmva.org/bursaries/sullivan-prize.html">The BMVA Sullivan Doctoral Thesis Prize Runner-Up</a></b></small>
        </p>

        <p class="pointers_sub">
        <small><B>Learning Multiple Dense Prediction Tasks from Partially Annotated Data</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Xialei Liu, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">CVPR 2022 (<b>Oral</b>)</small> <small>/ <a style="color:DarkRed" href="https://groups.inf.ed.ac.uk/vico/research/MTPSL/">Website</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_Multiple_Dense_Prediction_Tasks_From_Partially_Annotated_Data_CVPR_2022_paper.pdf">CVF</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Learning_Multiple_Dense_CVPR_2022_supplemental.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2111.14893.pdf">arXiv</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1nzQvBW5tCezFZLmiqqyGu7Qu8iFFYfaW/view">Video</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/MTPSL">Code</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1IyC2tiPuiB1UKsikMlh31dLKKY1JCbem/view">Poster</a> </small><br>
        <small><b><a style="color:DarkRed" href="https://twitter.com/CVPR/status/1539772091112857600">Best Paper Nominee</a></b> (33/8161)</small>
        </p>

        <p class="pointers_sub">
        <small><B>Cross-domain Few-shot Learning with Task-specific Adapters</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Xialei Liu, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">CVPR 2022</small> <small>/ <a style="color:DarkRed" href="https://groups.inf.ed.ac.uk/vico/research/TSA/">Website</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Domain_Few-Shot_Learning_With_Task-Specific_Adapters_CVPR_2022_paper.pdf">CVF</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Cross-Domain_Few-Shot_Learning_CVPR_2022_supplemental.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2107.00358.pdf">arXiv</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1FdEVmY2u0NbOtYv5ixsz72paJUbKYgjX/view">Video</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/URL">Code</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1CWBj_Z_Ig5G7S0uN9Za_46iLNqVh_SSQ/view">Poster</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Universal Representation Learning from Multiple Domains for Few-shot Classification</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Xialei Liu, Hakan Bilen</small>.<br>
        <!-- <small style="color:DarkRed">International Conference on Computer Vision</small>  -->
        <small style="color:DarkRed">ICCV 2021</small> <small>/ <a style="color:DarkRed" href="https://groups.inf.ed.ac.uk/vico/research/URL/">Website</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Universal_Representation_Learning_From_Multiple_Domains_for_Few-Shot_Classification_ICCV_2021_paper.pdf">CVF</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Li_Universal_Representation_Learning_ICCV_2021_supplemental.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2103.13841.pdf">arXiv</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1ZcKfg7K7Hy3s9g_MBTl2e3AeSfGDHTg3/view">Video</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/URL">Code</a> / <a style="color:DarkRed" href="https://drive.google.com/file/d/1El2S9G_qP2mTPeLhQy_-QcXInvqbNTJW/view">Poster</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Knowledge Distillation for Multi-task Learning</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">ECCV Workshop 2020</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/KD-MTL/KD-MTL.htm">Website</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2007.06889.pdf">arXiv</a> / <a style="color:DarkRed" href="https://www.youtube.com/watch?v=ckcFSi35Wuk&feature=youtu.be">Video</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/KD4MTL">Code</a></small>
        </p>

        <h1 style="font-size:1em; "> Other Papers </h1>

        <p class="pointers_sub">
        <small><B>Learning to Learn Relation for Important People Detection in Still Images</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>*, Fa-Ting Hong*, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">CVPR 2019</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/POINT/POINT.htm">Website</a> / <a style="color:DarkRed" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_to_Learn_Relation_for_Important_People_Detection_in_Still_CVPR_2019_paper.html">CVF</a> / <a style="color:DarkRed" href="https://WeiHongLee.github.io/papers/2305-supp.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/1904.03632.pdf">arXiv</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>One-pass Person Re-identiﬁcation by Sketched Online Discriminant Analysis</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Zhuowei Zhong, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">PR 2019</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/SoDA/SoDA.htm">Website</a> / <a style="color:DarkRed" href="https://arxiv.org/abs/1711.03368">arXiv</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>PersonRank: Detecting Important People in Images</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Benchao Li, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">FG 2018 (<b>Oral</b>)</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/PersonRank.htm">Website</a> / <a style="color:DarkRed" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8373835">paper</a> / <a style="color:DarkRed" href="https://uoe-my.sharepoint.com/personal/s1798461_ed_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fs1798461%5Fed%5Fac%5Fuk%2FDocuments%2Fdataset%2FMS%20DataSet%2Ezip&parent=%2Fpersonal%2Fs1798461%5Fed%5Fac%5Fuk%2FDocuments%2Fdataset">MS Dataset</a> / <a style="color:DarkRed" href="https://uoe-my.sharepoint.com/personal/s1798461_ed_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fs1798461%5Fed%5Fac%5Fuk%2FDocuments%2Fdataset%2FNCAA%20Dataset%2Ezip&parent=%2Fpersonal%2Fs1798461%5Fed%5Fac%5Fuk%2FDocuments%2Fdataset">NCAA Dataset</a></small> 
        </p>

        <p class="pointers_sub">
        <small><B>Correlation based Identity Filter: An Efficient Framework For Person Search</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Yafang Mao, Ancong Wu, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">ICIG 2017 (<b>Oral</b>)</small> <small>/ <a style="color:DarkRed" href="https://link.springer.com/chapter/10.1007/978-3-319-71607-7_22">paper</a></small><br>
        <small style="color:DarkRed"><b>Best Paper Award</b></small>
        </p>

        <p class="pointers_sub">
        <small><B>Learning to Impute: A General Framework for Semi-supervised Learning</B></small>.<br>
        <small><B><I>Wei-Hong Li</I></B>, Chuan-Sheng Foo, Hakan Bilen</small>.<br>
        <small style="color:DarkRed">Preprint 2019</small> <small>/ <a style="color:DarkRed"  href="https://WeiHongLee.github.io/Research/Learning-to-impute/Learning-to-impute.htm">Website</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/1912.10364.pdf">arXiv</a> / <a style="color:DarkRed" href="https://github.com/VICO-UoE/L2I">Code</a> </small>
        </p>

        <p class="pointers_sub">
        <small><B>Learning to Detect Important People in Unlabelled Images for Semi-supervised Important People Detection</B></small>.<br>
        <small>Fa-Ting Hong*, <B><I>Wei-Hong Li</I></B>*, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">CVPR 2020</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/Learning-to-Rank/Learning-to-Rank.htm">Website</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Learning_to_Detect_Important_People_in_Unlabelled_Images_for_Semi-Supervised_CVPR_2020_paper.html">CVF</a> / <a style="color:DarkRed" href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Hong_Learning_to_Detect_CVPR_2020_supplemental.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/abs/2004.07568">arXiv</a> / <a style="color:DarkRed" href="https://github.com/harlanhong/CVPR2020-Semi-Point">Code</a> / <a style="color:DarkRed" href="https://mail2sysueducn-my.sharepoint.com/personal/hongft3_mail2_sysu_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhongft3%5Fmail2%5Fsysu%5Fedu%5Fcn%2FDocuments%2Frelease%2Ddataset%2FEMS%2Ezip&parent=%2Fpersonal%2Fhongft3%5Fmail2%5Fsysu%5Fedu%5Fcn%2FDocuments%2Frelease%2Ddataset">EMS Dataset</a> / <a style="color:DarkRed" href="https://mail2sysueducn-my.sharepoint.com/personal/hongft3_mail2_sysu_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhongft3%5Fmail2%5Fsysu%5Fedu%5Fcn%2FDocuments%2Frelease%2Ddataset%2FENCAA%2Ezip&parent=%2Fpersonal%2Fhongft3%5Fmail2%5Fsysu%5Fedu%5Fcn%2FDocuments%2Frelease%2Ddataset">ENCAA Dataset</a> / <a style="color:DarkRed" href="https://WeiHongLee.github.io/Research/Learning-to-Rank/Basic/1842-poster.pdf">Poster</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Learning Relation Models to Detect Important People in Still Images</B></small>.<br>
        <small>Yu-Kun Qiiu, Fa-Ting Hong, <B><I>Wei-Hong Li</I></B>, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">TMM 2022</small> <small>/ <a style="color:DarkRed" href="https://ieeexplore.ieee.org/document/9907874">paper</a> </small>
        <!-- <small>/ <a style="color:DarkRed" href="">arXiv</a> / <a style="color:DarkRed" href="">Code</a></small> -->
        </p>

        <p class="pointers_sub">
        <small><B>MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection</B></small>.<br>
        <small>Fa-Ting Hong, Xuanteng Huang, <B><I>Wei-Hong Li</I></B>, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">ECCV 2020</small> <small>/ <a style="color:DarkRed" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580341.pdf">ECVA</a> / <a style="color:DarkRed" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580341-supp.pdf">supp</a> / <a style="color:DarkRed" href="https://arxiv.org/pdf/2007.09833.pdf">arXiv</a> / <a style="color:DarkRed" href="https://github.com/Huangxt57/MINI-Net">Code</a></small>
        </p>

        <p class="pointers_sub" style="text-align:left">
        <small><B>Sketch metric learning</B></small>.<br>
        <small>Yuting Mai, <B><I>Wei-Hong Li</I></B>, Yongyi Tang, Xixi Bi, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">IJCNN 2016</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/papers/Sketch_Metric_Learning.pdf">paper</a></small>
        </p>

        <p class="pointers_sub">
        <small><B>Towards Photo-Realistic Visible Watermark Removal with Conditional Generative Adversarial Networks</B></small>.<br>
        <small>Xiang Li, Chan Lu, Danni Cheng, <B><I>Wei-Hong Li</I></B>, Mei Cao, Bo Liu, Jiechao Ma, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">ICIG 2019</small> <small>/ <a style="color:DarkRed" href="https://arxiv.org/pdf/1905.12845.pdf">paper</a></small>
        </p>
          
        <p class="pointers_sub">
        <small><B>Large-Scale Visible Watermark Detection and Removal with Deep Convolutional Networks</B></small>.<br>
        <small>Danni Cheng, Xiang Li, <B><I>Wei-Hong Li</I></B>, Chan Lu, Fake Li, Hua Zhao, Wei-Shi Zheng</small>.<br>
        <small style="color:DarkRed">PRCV 2018 (<b>Oral</b>)</small> <small>/ <a style="color:DarkRed" href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-03338-5_3.pdf">paper</a> / <a style="color:DarkRed" href="https://pan.baidu.com/s/1c3MuJ90jVcKJm2Gp-wwzNQ#list/path=%2F">Dataset (baidu)</a> / <a style="color:DarkRed" href="https://uoe-my.sharepoint.com/:f:/g/personal/s1798461_ed_ac_uk/Er63ULEdBlhLt7fxxQuXPwEBVC-JSZUqvgRyAFMqvhsIrg?e=zXGPBd">Dataset (onedrive)</a></small>
        </p>
          
        <p class="pointers_sub" style="text-align:left">
        <small><B>A Delaunay-Based Temporal Coding Model for Micro-expression Recognition</B>.<br>
        Zhaoyu Lu and Ziqi Luo and Huicheng Zheng and Jikai Chen and <B><I>Wei-Hong Li</I></B>.</small><br>
        <small style="color:DarkRed">ACCV Workshop 2014</small> <small>/ <a style="color:DarkRed" href="https://WeiHongLee.github.io/papers/A_Delaunay-Based_Temporal_Coding_Model_for_Micro-expression_Recognition.pdf">paper</a></small>
        </p>
        <h1 style="font-size:1em; "> <a href="https://scholar.google.com/citations?hl=zh-CN&user=xKKsIxcAAAAJ&view_op=list_works&authuser=1&sortby=pubdate">full publications</a> </h1>
        </div>

<!-- <span id="buffer-extension-hover-button" style="display: none;position: absolute;z-index: 8675309;width: 100px;height: 25px;background-image: url(chrome-extension://noojglkidnpfjbincgijbaiedldjfbhh/data/shared/img/buffer-hover-icon@1x.png);background-size: 100px 25px;opacity: 0.9;cursor: pointer;"></span></body></html> -->
</small></small></body></html>
